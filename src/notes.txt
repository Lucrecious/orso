
## folding-functions
Okay... Time to do some explaining. This is a complicated one. Let's begin with the context.

In a regular compiler that does not care about compile time evaluation of arbitrary expressions,
they need not worry about *when* a function's body has been analyzed[1], only that it needs to be
analyzed *eventually*. This is because in a regular compile, the assumption is that all functions
run *after* everything already has been analyzed *and* compiled. This means that once we know a
function's address in memory, every time we need to reference the function we can safely use its address.
This is true even if we haven't fully compiled a function's body too. This is useful for recursion because
while analyzing (mainly constant folding) a function's body, the body might call a function that references
the current function's body that you're analyzing. You can simply place a function's address where ever it's
references, and continue analyzing. This is an example of not having to care *when* a function's body gets analyzed
since it doesn't interfere with the rest of the compilation. 

When a language cares about compile time expression evaluation (CTEE) things get a little more complicated... Despite 
only needing to solve *when* it is possible to do CTEE this one problem is very complex to solve.

While solving recursion was straight forward with a regular compiler, it's not as clear with a CTEE compiler. Any
expression composed of only constant values can be CTEEed. This is different from constant folding which reduces
but doesn't run functions (even if its parameters are constants). This means functions can be ran while certain
portions of the program are still compiling. Again, this is in contrast while a regular compiler where *all* analysis
is done before *any* function is called ever.

So *when* can we perform CTEE? First, all entities of the expression being folded must
     1. be constant
     2. a mutable variable that was declared in the same fold level as the expression

The first requirement is trivial. You cannot fold an expression if the entities do not hold a concrete value.
The second is less obvious. In Orso, entities can be defined inside blocks, and blocks are expressions. This means
that the declarations in the block get run in order, inlined. Unlike a function that can be called at any time.
This means blocks undergoing constant folding can still access variables that were created during the fold level
since everything in the same fold level must be able to run on its own (assuming all constants are resolved)

The constant rule (1) only applies outside function boundaries. When resolving the inside of a function
all declarations are localized to that function, and the function cannot access local variables at higher
scope. So while the (2) rule must still apply, functions already restrict access to mutable variables outside
the their body. That said, the second rule still comes into play when a resolving a function under any folding level
and trying to access the global state. The global state starts at fold level 0, and so if a function is being folded it
will never be able to access the global state (i.e. must be a pure function). 

Rule A: Remember that when folding happens, the expression *must* be able to run on its own aside from constants.
i.e. the expression can be put into a compiler with the given folded constants and compile perfect.

Another issues arises when you can nest folds. i.e. in the following case

     T :: #fold foo();
      
     foo :: () -> s32 {
         return T;
     };

Even though T is a constant, because T requires the body of foo to fold, this creates a circular dependency.
However, how do you know when there is a circualr dependency like this or we are just recursioning and only
need the function address? If we are currently resolving the body of the function and we increase the fold level
this implies that whatever is being folded must be able to run on its own (as per Rule A). If we need to call
a function we need to check if its currently being resolved at a different fold level - because if it is, then
we cannot call the function during compile time.

Anyways, that's what the we are doing below. If the function definition was processed during
the same fold level that we are currenty on, then we are good. We are doing a regular function call.

However, if we are on a different fold level, we need to check if the definition is in the process of
being resolved because it needs to be compiled before running, and since it's in the middle of being resolved
it cannot be compiled won't ever be due to the circular dependency.

    [1] by analyzed I mean type checked, type flowed, constants folded, etc. i.e. getting the ast ready for codegen.


## struct-resolution

At this point in the analysis, we know the type for this struct definition is unknown. Otherwise
we wouldn't be here trying to resolve it (expression calls this only if the ast is unresolved)
It's possible for a struct to be defined like this:

          Foo :: struct {
              i: s32;
              foo: Foo;
          };

Despite this being an invalid type (since this creates an infinitely sized struct) I can only find out
after I figure out *what* Foo is. The first time Foo's assigned value is being analyzed is also the first
time the analyzer enters the struct and encounters Foo. At this point, it is not obvious what Foo is because
we are actively in the process of figuring that out!

          Foo :: #fold if PLATFORM == WIN32_ then
              struct {
                  platform := WINDOWS;
                  foo: Foo;
              };
          else
              struct {
                  platform := UNIX;
                  foo: Foo;
              };


This looks dumb, and it is, but it's also valid. And it means we cannot know what Foo is until *after* we have
analyzed the `if` expression (since struct's platform's default value can be WINDOWS or UNIX).
And for that, we need to loop around and figure out the type for Foo.

         Foo :: #fold generate_foo();

         generate_foo :: (n: s32) -> struct { x: s32; Foo: foo; } { ... }
         // this fails because of this dependency chain
         // #fold generate_foo() => (n: s32) -> struct { x: s32, foo: Foo; } => struct { x: s32, foo: Foo; }
         //      => Foo -> #fold generate_foo() <== this generates a cycle error

Here's another weird example

         Foo :: #fold generate_foo();

         generate_foo :: (n: s32) -> struct { x: s32; } {
             return struct {
                 x := Foo.x;
             };
         };

         This creates a circular dependency like this:
         #fold generate_foo() [fold level: 1] => (n: s32) -> struct { x: s32; } { ... }
             => struct { x := Foo.x } => Foo.x => Foo => #fold generate_foo() [fold level: 2]
                 => (n: s32) -> struct { x: s32 } { ... } <=== ...
                     // circular dependency since generate_foo's body is in the dependency list still
                     // because constructed but on a different fold level

Important to note that if the dependency chain started with generate_foo instead of Foo, the circular
dependency would be resolved a little earlier. 
                              
Here's another *working* example however

         Foo :: #fold generate_foo(10);

         generate_foo() :: (n: s32) -> struct { x: s32; } {
             return struct {
                 x := n * 10;
             };
         };

In the above example, Foo *should* be a struct like this: struct { x := 100; }, I think that
makes sense.

Later on, it will be possible to do this based on compile type parameters

         Vector2i :: #fold generate_vector2(s32, 0);
         Vector2f :: #fold generate_vector2(f32, 0.0);
         
         // for now the `!` means the parameter in the function call must be a compile time constnat
         generate_foo(!t: type, default_value: t) -> struct { x: t; y: t; } {
             return struct {
                 x := default_value;
                 y := default_value;
             };
         };

This is an interesting one because this means that generic structs already exist in a similar way they do
in C++ with templates or C with macros. Howver, there will be actual generic structs.

Here's another complicated one I guess

         Foo :: struct {
             MyValue :: 10;
             value := #fold generate_value();
         };

         generate_value :: () -> s32 {
             return Foo.MyValue;
         };

This *should* work I guess?
struct { MyValue:: 10; value := #fold generate_value(); }; 
     => #fold generate_value()
         => () -> s32 { return Foo.MyValue; }
             => Foo.MyValue => Foo // At this point, get_resolved_def_by_identifier will find
                                   // Foo points to the "resolved" struct. But at this point, its still
                                   // potentially incomplete (in this case it is because while we are figuring out
                                   // what `value` is we are figuring out what is `Foo.MyValue`).
                                   // However, this should still work because MyValue is a constant that does
                                   // does not have a ciruclar dependency.

This gets more complicated when accessing a non-constant value:

         Foo :: struct {
             value1 := 10;
             value2 := #fold generate_value();
         };

         (1)
         generate_value :: () -> s32 {
             foo := Foo();
             return foo.value1;
         };

         (2)
         generate_value :: () -> s32 {
             foo := Foo();
             return foo.value2;
         };

For (1) this can be "designed" to work since its only accessing resolved fields. In the interpreter, 
we can easily create an "incomplete" type just for the vm run. And since there were no causes for 
circular dependencies, the codegen should be fine and the resulting runtime should be as well. This is 
because the analyzer is so pathological. We know if we resolve a branch of the ast, all the dependencies
under it are ready. But we will not make this work.

## circular-dependencies
We stop looking for circular dependencies at function boundaries. This took me a while to reason but this
is okay to do even with constant folding. We can think about compiling a progarm with a language like in two different ways.
Right now, the way the compiler works is through a principle I call "eager resolution". Which means that when a problem
comes up during static analysis (i.e. an unresolved expression) we immediately go to resolve it. For example, a problem
that may occur is that while trying to resolve the type of an definition, you need to resolve an expression, but to resolve
that expression, you might need to perform a folding operation. My compiler walks down the line of dependencies and resolves
all the problems that come along the way.

Circular dependencies are easily to avoid with non-functions but recursion makes things a little weird. Since a function
can dependent on itself. We can have a weird situation like this:
         
         foo();

         foo :: null or (n: s32) -> void {
             if n < 0 {
                 return 0;
             };
             
             return foo(n - 1);
         };

Technically... this should run just fine right? null or function literal, and foo being a constant, means foo 
should hold the address to the function expression on the right side of the OR. This is allowed and my language
and should be handled.

The way my dependency system works is through expressions. Expressions always, always, always rely on other expressions.
In the case above, when foo(n - 1) is about to be analyzed this is how the dependency chain looks like.

         NOTE: so far only the following things create dependencies.
             - The top level type of a declaration
             - The top level expression of a declaration
             - Function definitons
           this is why the dependency chain is not foo() -> foo, even though foo() technically depends on foo.
           I could make dependencies from each and all dependencies but that's way too much and I'm pretty sure
           is unnecessary since a lot of dependencies would be redundant. These are supposed to skip redundant
           dependencies.

         foo() => // this would not be in the chain, but it's here because it's what kicks off the chain
             (null or (n: s32) -> void { ... }) => // foo declaration expression
                 (n: s32) -> void { ... } => // function definition
                     foo(n - 1) // this would not be in the chain, but again, it kicks off the above since foo is not resolved.

At this point, foo does not have the function address yet during the anaysis! It's still in the process of being resolved...
BUT once its expression is resolved, it will be good go! At this point, we STILL don't really know what foo is!


If I were to naively check of dependencies the chain would contain to look like this (starting from foo(n - 1))

         foo(n - 1) => 
             (null or (n: s32) -> void { ... }) => // foo declaration expression

Then we've come back and hit an expression we haven't resolved yet (remember the or wasn't resolved
because we were in the middle of resolving (n: s32) -> void { ... } due to the principle of eager resolution)
and therefore have a circular dependency, right? However, I found a different to do things that could work.

Imagine if instead of resolving the function block right away, I actually waited until the declaration resolved
before trying to analysis the function body! Well, then I can use the address of the function, and as long as I
store the scope and body, I can put it in a queue to analysis later. After I am sure I can safely resolve the
function body, then I will. This wouldn't have to deal with circular dependencies.

This truth is that the above *should* work (although I haven't tested it) because of part of the proof reasoning below
but this is, in truth, a little annoying to implement. It means I need to sort of break the recursion scheme and
eager resolution princple. I have to allocate more memory on the heap. It's just not an ideal soluton and makes
things much messier...

However, the other reasoning of the proof below suggests that the order in which I resolve expressions shouldn't matter.
So I can't naively just walk up the dependency chain and check for duplicates, I need to be a tiny bit smarter. Since the
order in which I resolve things doesn't matter as long as I have the data there (the scope chains basically).

I noticed that if I analyse a function body's when I know it's safe, the circular dependencies *must* be either
be localized inside the function OR will hit a circular dependency of constants on the outer layer - but in either
case - these circular dependencies will be a result of unused entities trying to be resolved from *inside* the function.

Therefore instead of a simply looking for duplicate expression pointers in the entire dependency chain, I start
backwards and when I see a dependency to a function definition, I stop looking. This mimics the behavior of the
ideal solution but allows me not break any recursion.
